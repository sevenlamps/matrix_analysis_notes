\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}

\usepackage{tikz-cd}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\setcounter{section}{-1}

% \usepackage{multicol}
% \setlength{\columnsep}{1cm}

\newtheorem*{lemma}{Lemma}
\newtheorem*{example}{Example}
\newtheorem*{remarks}{Remarks}
\newtheorem*{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}
    \section{Review and Miscellanea}
    This chapter adopts Hoffman's definition for clearer presentations.
    % \setcounter{subsection}{-1}
    \subsection{Vector Spaces}
    A finite dimensional vector space is the fundamental setting for matrix analysis.
    % scalar field
    \subsubsection{Scalar Field}
    \begin{definition}
        Field \(\mathbb{F}\) is the set together with addition and
        multiplication. A field must satisfy the following properties:
        \begin{enumerate}
            \item Addition is commutative, \[x+y=y+x\] for all \(x\) and \(y\) in \(\mathbb{F}\).
            \item Addition is associative, \[x+(y+z)=(x+y)+z\] for all \(x\), \(y\) and \(z\) in \(\mathbb{F}\).
            \item There is a unique element 0 (zero) in \(\mathbb{F}\) such that \(x+0=x\), for every \(x\) in \(\mathbb{F}\).
            \item To each \(x\) in \(\mathbb{F}\) there corresponds a unique element \((-x)\) in \(\mathbb{F}\) such that \(x + (-x) = 0\).
            \item Multiplication is commutative, \[xy=yx\] for all \(x\) and \(y\) in \(\mathbb{F}\).
            \item Multiplication is associative, \[x(yz)=(xy)z\] for all \(x\), \(y\), and \(z\) in \(\mathbb{F}\).
            \item There is a unique element 1 (one) in \(\mathbb{F}\) such that \(x1=x\), for every \(x\) in \(\mathbb{F}\).
            \item To each non-zero \(x\) in \(\mathbb{F}\) there corresponds a unique element \(x^{-1}\) (or \(1/x\)) in \(\mathbb{F}\) such that \(xx^{-1}=1\).
            \item Multiplication distributes over addition; that is, \(x(y+z)=xy+xz\) for all \(x\), \(y\), and \(z\) in \(\mathbb{F}\).
        \end{enumerate}
    \end{definition}
    \begin{remarks}
        If \(\mathbb{F}\) is a field, it may be possible to add the unit 1 to itself
        a finite number of times and obtain 0: \[1+1+\cdots+1=0\] That does not happen in
        the complex field (or in any subfield thereof). If it does happen in a field \(\mathbb{F}\),
        then the least \(n\) such that the sum of \(n\) 1's is 0 is called the characteristic
        of the field \(\mathbb{F}\). If this does not happen in \(\mathbb{F}\), then it it
        is called a field of characteristic zero.
    \end{remarks}

    % vector space
    \subsubsection{Vector spaces}
    \begin{definition}
        A vector space (or linear space) consists of the following:
        \begin{enumerate}
            \item a field \(\mathbb{F}\) of scalars;
            \item a set \(\mathbb{V}\) of objects, called vectors;
            \item a rule (or operation), called vector addition, which associates each pair of vectors \(\vec{\alpha}\), \(\vec{\beta}\) in \(\mathbb{V}\) a vector \(\vec{\alpha}+\vec{\beta}\) in \(\mathbb{V}\), called the sum of \(\vec{\alpha}\) and \(\vec{\beta}\), in such a way that
            \begin{enumerate}
                \item addition is commutative, \(\vec{\alpha}+\vec{\beta}=\vec{\beta}+\vec{\alpha}\);
                \item addition is associative, \(\vec{\alpha}+(\vec{\beta}+\vec{\gamma})=(\vec{\alpha}+\vec{\beta})+\vec{\gamma}\);
                \item there is a unique vector 0 in \(\mathbb{V}\), called the zero vector, such that \(\vec{\alpha}+0=\vec{\alpha}\) for all \(\vec{\alpha}\) in \(\mathbb{V}\);
                \item for each vector \(\vec{\alpha}\) in \(\mathbb{V}\) there exists a unique vector \(-\vec{\alpha}\) in \(\mathbb{V}\) such that \(\vec{\alpha}+(-\vec{\alpha})=0\).
            \end{enumerate}
            \item a rule (or operation), called scalar multiplication, which associates with each scalar \(c\) in \(\mathbb{F}\) and vector \(c\vec{\alpha}\) in \(\mathbb{F}\), called the product of \(c\) and \(\alpha\), in such a way that
            \begin{enumerate}
                \item \(1\vec{\alpha}=\vec{\alpha}\) for every \(\vec{\alpha}\) in \(\mathbb{V}\);
                \item \((c_1c_2)\vec{\alpha}=c_1(c_2 \vec{\alpha})\);
                \item \(c(\vec{\alpha}+\vec{\beta})=c \vec{\alpha} + c \vec{\beta}\);
                \item \((c_{1}+c_{2})\vec{\alpha} = c_{1} \vec{\alpha} + c_{2} \vec{\alpha}\).
            \end{enumerate}
        \end{enumerate}
    \end{definition}

    % subspaces, span, and linear combinations
    \subsubsection{Subspaces, span, and linear combinations}
    \begin{definition}
        A vector \(\vec{\beta}\) in \(\mathbb{V}\) is said to be a linear combination of
        the vectors \(\vec{\alpha_{1}}, \cdots, \vec{\alpha_{n}}\) in \(\mathbb{V}\) provided there
        exists scalars \(c_{1}, \cdots, c_{n}\) in \(\mathbb{F}\) such that 
        \begin{equation*}
            \begin{split}
                \vec{\beta}&=c_{1} \vec{\alpha_{1}} + \cdots + c_{n} \vec{\alpha_n} \\ &= \sum_{i=1}^{n} c_{i} \vec{\alpha}_{i}
            \end{split}
        \end{equation*}
    \end{definition}

    \begin{definition}
        Let \(\mathbb{V}\) be a vector space over the field \(\mathbb{F}\). A subspace of
        \(\mathbb{V}\) is a subset \(\mathbb{W}\) of \(\mathbb{V}\) which is itself a vector
        space over \(\mathbb{F}\) with the operations of vector addition and scalar multiplication
        on \(\mathbb{V}\).
    \end{definition}
    \begin{theorem}
        A non-empty subset \(\mathbb{W}\) of \(\mathbb{V}\) is a subspace of \(\mathbb{V}\)
        if and only if for each pair of vectors \(\vec{\alpha}\), \(\vec{\beta}\) in \(\mathbb{W}\)
        and each scalar \(c\) in \(\mathbb{F}\) the vector \(c \vec{\alpha} + \vec{\beta}\) is
        again in \(\mathbb{W}\).
    \end{theorem}
    \begin{proof}
        Suppose \(\mathbb{W}\) is a non-empty subset of \(\mathbb{V}\) such that 
        \(c \vec{\alpha} + \vec{\beta}\) belongs to \(\mathbb{W}\) for all vectors \(\vec{\alpha}\),
        \(\vec{\beta}\) in \(\mathbb{W}\) and all scalars \(c\) in \(\mathbb{F}\). Since \(\mathbb{W}\)
        is non-empty, there is a vector \(\vec{\rho}\) in \(\mathbb{W}\), and hence 
        \((-1)\vec{\rho} + \vec{\rho}= 0\) is in \(\mathbb{W}\). Then if \(\vec{\alpha}\)
        is any vector in \(\mathbb{W}\) and c any scalar, the vector \(c \vec{\alpha} = c \vec{\alpha} + 0\)
        is in \(\mathbb{W}\). In particular, \((-1) \vec{\alpha} =- \vec{\alpha}\) is in \(\mathbb{W}\).
        Finally, if \(\vec{\alpha}\) and \(\vec{\beta}\) are in \(\mathbb{W}\), then 
        \(\vec{\alpha} + \vec{\beta}=1 \vec{\alpha} + \vec{\beta}\) is in \(\mathbb{W}\).
        Thus \(\mathbb{W}\) is a subspace of \(\mathbb{V}\).
        Conversely, if \(\mathbb{W}\) is a subspace of \(\mathbb{V}\), \(\vec{\alpha}\)
        and \(\vec{\beta}\) are in \(\mathbb{W}\), and \(c\) is a scalar, certainly
        \(c \vec{\alpha} + \vec{\beta}\) is in \(\mathbb{W}\).
    \end{proof}
    \begin{lemma}
        If \(\mathbf{A}\) is an \(m \times n\) matrix over \(\mathbb{F}\) and 
        \(\mathbf{B}\), \(\mathbf{C}\) are \(n \times p\) matrices over \(\mathbb{F}\), then
        \begin{equation}
            \label{}
            \mathbf{A}(d \mathbf{B} + \mathbf{C}) = d(\mathbf{A} \mathbf{B}) + \mathbf{A}\mathbf{C}
        \end{equation}
        for each scalar \(d\) in \(\mathbb{F}\).
    \end{lemma}
    \begin{proof}
        \begin{equation*}
            \begin{split}
                [\mathbf{A}(d \mathbf{B} + \mathbf{C})]_{ij} &= \sum_{k=1}^{n} a_{ik}(d \mathbf{B} + \mathbf{C})_{kj}\\
                                                             &= \sum_{k=1}^{n} a_{ik}(d b_{kj} + c_{kj})\\
                                                             &= \sum_{k=1}^{n} (d a_{ik} b_{kj} + a_{ik} c_{kj})\\
                                                             &= d \sum_{k=1}^{n} a_{ik} b_{kj} + \sum_{k=1}^{n} a_{ik} c_{kj}\\
                                                             &= d(\mathbf{A} \mathbf{B})_{ij} + (\mathbf{A} \mathbf{C})_{ij}\\
                                                             &= (d (\mathbf{A} \mathbf{B}) + \mathbf{A} \mathbf{C})_{ij}
            \end{split}
        \end{equation*}
    \end{proof}
    \begin{theorem}
        Let \(\mathbb{V}\) be a vector space over the field \(\mathbb{F}\). The intersection
        of any collection of subspcaces of \(\mathbb{V}\) is a subspace of \(\mathbb{V}\).
    \end{theorem}
    \begin{proof}
        Let \({\mathbb{W}_a}\) be a collection of subspaces of \(\mathbb{V}\), and let 
        \(\mathbb{W}=\cap_{a} \mathbb{W}_{a}\) be their intersection. Recall that 
        \(\mathbb{W}\) is defined as the set of all elements belonging to every \(\mathbb{W}_a\).
        Since each \(\mathbb{W}_{a}\) is a subspace, each contains the zero vector. Thus 
        zero is inthe intersection of \(\mathbb{W}\), and \(\mathbb{W}\) is non-empty. Let
        \(\vec{\alpha}\) and \(\vec{\beta}\) be vectors in \(\mathbb{W}\) and let \(c\) be a scalar.
        By definition of \(\mathbb{W}\), both \(\vec{\alpha}\) and \(\vec{\beta}\) belong to each
        \(\mathbb{W}_{a}\), and because each \(\mathbb{W}_{a}\) is a subspace, the vector \((c \vec{\alpha}+\vec{\beta})\)
        is in every \(\mathbb{W}_a\). Thus \(c \vec{\alpha} + \vec{\beta}\) is in \(\mathbb{W}\).
        By Theorem 1, \(\mathbb{W}\) is a subspace of \(\mathbb{V}\).
    \end{proof}
    \begin{definition}
        Let \(\mathbb{S}\) be a set of vectors in a vector space \(\mathbb{V}\). The subspace
        spanned by \(\mathbb{S}\) is defined to be the intersection \(\mathbb{W}\) of all 
        subspaces of \(\mathbb{V}\) which contain \(\mathbb{S}\). When \(\mathbb{S}\) is a 
        finite set of vectors, \(\mathbb{S} = {\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n}\), we 
        shall simply call \(\mathbb{W}\) the subspace spanned by the vectors \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\).
    \end{definition}
    \begin{theorem}
        The subspace spanned by a non-empty subset \(\mathbb{S}\) of a vector space \(\mathbb{V}\)
        is the set of all linear combinations of vectors in \(\mathbb{S}\).
    \end{theorem}
    \begin{proof}
        Let \(\mathbb{W}\) be the subspace spanned by \(\mathbb{S}\). Then each linear combination
        \begin{equation*}
            \vec{\alpha} = x_{1} \vec{\alpha}_1 + x_{2} \vec{\alpha}_2 + \cdots + x_{m} \vec{\alpha}_m
        \end{equation*}
        of vector \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_m\) in \(\mathbb{S}\)
        is clearly in \(\mathbb{W}\). Thus \(\mathbb{W}\) contains the set \(\mathbb{L}\) of all 
        linear combinations of vectors in \(\mathbb{S}\). The set \(\mathbb{L}\), on the other hand, 
        contains \(\mathbb{S}\) and is non-empty. If \(\vec{\alpha}\), \(\vec{\beta}\) belong
        to \(\mathbb{L}\) then \(\vec{\alpha}\) is a linear combination,
        \begin{equation*}
            \vec{\alpha} = x_{1} \vec{\alpha}_1 + x_{2} \vec{\alpha}_2 + \cdots + x_{m} \vec{\alpha}_m
        \end{equation*}
        of vectors \(\vec{\alpha}_i\) in \(\mathbb{S}\), and \(\vec{\beta}\) is a linear combination,
        \begin{equation*}
            \vec{\beta} = y_{1} \vec{\beta}_1 + y_{2} \vec{\beta}_2 + \cdots + y_{n} \vec{\beta}_n
        \end{equation*}
        of vectors \(\vec{\beta}_i\) in \(\mathbb{S}\). For each scalar \(c\), 
        \begin{equation*}
            c \vec{\alpha} + \vec{\beta} = \sum_{i=1}^{m} c x_{i} \vec{\alpha}_i + \sum_{j=1}^{n} y_{i} \vec{\beta}_i
        \end{equation*}
        Hence \(c \vec{\alpha} + \vec{\beta}\) belongs to \(\mathbb{L}\). Thus \(\mathbb{L}\)
        is a subspace of \(\mathbb{V}\).
        Now we have shown that \(\mathbb{L}\) is a subspace of \(\mathbb{V}\) which contains
        \(\mathbb{S}\), and also that any subspace which contains \(\mathbb{S}\) contains \(\mathbb{L}\). 
        It follows that \(\mathbb{L}\) is the intersection of all subspaces containing \(\mathbb{S}\), 
        i.e., that \(\mathbb{L}\) is the subspace spanned by the set \(\mathbb{S}\).
    \end{proof}
    \begin{definition}
        If \(\mathbb{S}_1, \mathbb{S}_2, \cdots, \mathbb{S}_n\) are subsets of a vector
        space \(\mathbb{V}\), the set of all sums
        \begin{equation*}
            \vec{\alpha}_1 + \vec{\alpha}_2 + \cdots + \vec{\alpha}_k
        \end{equation*}
        of vectors \(\vec{\alpha}_{i}\) in \(\mathbb{S}_i\) is called the sum of the subsets
        \(\mathbb{S}_1, \mathbb{S}_2, \cdots, \mathbb{S}_k\) and is
        denoted by 
        \[\mathbb{S}_1+\mathbb{S}_2+\cdots+\mathbb{S}_k\]
        or by 
        \[\sum_{i=1}^{k} \mathbb{S}_{i}\]
    \end{definition}

    % linear dependence and linear independence
    \subsubsection{Linear dependence and linear independence}
    \begin{definition}
        Let \(\mathbb{V}\) be a vector space over \(\mathbb{F}\). A subset \(\mathbb{S}\)
        of \(\mathbb{V}\) is said to be linearly dependent (or simply, dependent) if there 
        exists distinct vectors \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\)
        in \(\mathbb{S}\) and scalars \(c_1, c_2, \cdots, c_n\) in \(\mathbb{F}\), not all
        of which are \(0\), such that 
        \begin{equation*}
            c_1 \vec{\alpha}_1 + c_2 \vec{\alpha}_2 + \cdots + c_n \vec{\alpha}_n = 0
        \end{equation*}
        A set which is not linearly dependent is called linearly independent. If the set 
        \(\mathbb{S}\) contains only finitely many vectors \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots \vec{\alpha}_n\),
        we someimtes say that \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\) is 
        dependent (or independent) instead of saying \(\mathbb{S}\) is dependent (or independent).
    \end{definition}

    % basis
    \subsubsection{Basis}
    \begin{definition}
        Let \(\mathbb{V}\) be a vector space. A basis for \(\mathbb{V}\) is linearly
        independent set of vectors in \(\mathbb{V}\) which spans the space \(\mathbb{V}\).
        The space \(\mathbb{V}\) is finite-dimensional if it has a finite basis.
    \end{definition}
    \begin{theorem}
        Let \(\mathbb{V}\) be a vector space which is spanned by a finite set of vectors 
        \(\vec{\beta}_1, \vec{\beta}_2, \cdots, \vec{\beta}_m\). Then any independent set 
        of vectors in \(\mathbb{V}\) is finite and contains no more than \(m\) elements.
    \end{theorem}
    \begin{proof}
        To prove the theorem it suffices to show that every subset \(\mathbb{S}\) of
        \(\mathbb{V}\) which contains more than \(m\) vectors is linearly dependent.
        Let \(\mathbb{S}\) be such a set. In \(\mathbb{S}\) there are distinct vectors 
        \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\) where \(n > m\). 
        Since \(\vec{\beta}_1, \cdots, \vec{\beta}_m\) span \(\mathbb{V}\), there exist
        scalars \(A_{ij}\) in \(\mathbb{F}\) such that 
        \begin{equation*}
            \vec{\alpha}_j = \sum_{i=1}^{m} A_{ij} \vec{\beta}_i
        \end{equation*}
        For any \(n\) scalars \(x_1, x_2, \cdots, x_n\), we have 
        \begin{equation*}
            \begin{split}
                x_1 \vec{\alpha}_1 + \cdots + x_n \vec{\alpha}_n &= \sum_{j=1}^{n} x_{j} \vec{\alpha}_j \\
                                                                 &= \sum_{j=1}^{n} x_{j} \sum_{i=1}^{m} A_{ij} \vec{\beta}_i \\
                                                                 &= \sum_{j=1}^{n} \sum_{i=1}^{m} (A_{ij} x_j) \vec{\beta}_{i} \\
                                                                 &= \sum_{i=1}^{m} (\sum_{j=1}^{n} A_{ij}x_{j}) \vec{\beta}_{i}
            \end{split}
        \end{equation*}
        Since \(n > m\), it implies there exist scalars \(x_1, x_2, \cdots, x_n\) not all \(0\) such that 
        \begin{equation*}
            \sum_{j=i}^{n} A_{ij}x_j=0, \ \ \ \ \ \ 1 \leq i \leq m.
        \end{equation*}
        Hence \(x_1 \vec{\alpha}_1 + x_2 \vec{\alpha}_2 + \cdots + x_n \vec{\alpha}_n = 0\). 
        This allows that \(\mathbb{S}\) is a linearly dependent set.
    \end{proof}
    \setcounter{corollary}{0}
    \begin{corollary}
        If \(\mathbb{V}\) is a finite-dimensional vector space, then any two bases of
        \(\mathbb{V}\) have the same (finite) number of elements.
    \end{corollary}
    \begin{proof}
        Since \(\mathbb{V}\) is finite-dimensional, it has the finite basis
        \begin{equation*}
            {\vec{\beta}_1, \vec{\beta}_2, \cdots, \vec{\beta}_m}
        \end{equation*}
        By Theorem 4 every basis of \(\mathbb{V}\) is finite and contains no more than 
        \(m\) elements. Thus if \({\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n}\)
        is a basis, \(n \leq m\). By the same argument, \(m \leq n\). Hence \(m=n\).
    \end{proof}
    \begin{corollary}
        Let \(\mathbb{V}\) be a finite-dimensional vector space and let \(n=\dim{\mathbb{V}}\). Then 
        \begin{enumerate}
            \item any subset of \(\mathbb{V}\) which contains more than \(n\) vectors is linearly dependent;
            \item no subset of \(\mathbb{V}\) which contains fewer than \(n\) vectors can span \(\mathbb{V}\).
        \end{enumerate}
    \end{corollary}
    \begin{lemma}
        Let \(\mathbb{S}\) be a linearly independent subset of a vector space \(\mathbb{V}\).
        Suppose \(\vec{\beta}\) is a vector in \(\mathbb{V}\) which is not in the subspace
        spanned by \(\mathbb{S}\). Then the set obtained by adjoining \(\vec{\beta}\) to 
        \(\mathbb{S}\) is linearly independent.
    \end{lemma}
    \begin{proof}
        Suppose \(\vec{\alpha}_1, \cdots, \vec{\alpha}_m\) are distinct vectors in \(\mathbb{S}\) and that 
        \begin{equation*}
            c_1 \vec{\alpha}_1 + \cdots + c_m \vec{\alpha}_m + b \vec{\beta} = 0
        \end{equation*}
        Then \(b=0\); for otherwise,
        \begin{equation*}
            \vec{\beta} = (-c_1/b)\vec{\alpha}_1 + \cdots + (-c_m/b)\vec{\alpha}_m
        \end{equation*}
        and \(\vec{\beta}\) is in the subspace spanned by \(\mathbb{S}\). Thus 
        \(c_1 \vec{\alpha}_1 + \cdots + c_m \vec{\alpha}_m = 0\), and since \(\mathbb{S}\) 
        is linearly independent set each \(c_i=0\).
    \end{proof}
    \begin{theorem}
        If \(\mathbb{W}\) is a subspace of a finite-dimensional vector space \(\mathbb{V}\), 
        every linearly independent subset of \(\mathbb{W}\) is finite and is part of a (finite) 
        basis for \(\mathbb{W}\).
    \end{theorem}
    \begin{proof}
        Suppose \(\mathbb{S}_0\) is a linearly independent subset of \(\mathbb{W}\). If 
        \(\mathbb{S}\) is a linearly independent subset of \(\mathbb{W}\) containing \(\mathbb{S}_0\),
        then \(\mathbb{S}\) is also a linearly independent subset of \(\mathbb{V}\); since 
        \(\mathbb{V}\) is finite-dimensional, \(\mathbb{S}_0\) contains no more than \(\dim{V}\) elements.

        We extend \(\mathbb{S}_0\) to a basis for \(\mathbb{W}\), as follows. If \(\mathbb{S}_0\) 
        spans \(\mathbb{W}\), then \(\mathbb{S}_0\) is a basis for \(\mathbb{W}\) and we are done.
        If \(\mathbb{S}_0\) does not span \(\mathbb{W}\), we use the preceding lemma to find a 
        vector \(\vec{\beta}_1\) in \(\mathbb{W}\) such that the set \(\mathbb{S}_1=\mathbb{S}_0 \cup {\{\vec{\beta}_1\}}\) 
        is independent. If \(\mathbb{S}_0\) spans \(\mathbb{W}\), fine. If not, apply the lemma 
        to obtain a vector \(\vec{\beta}_2\) in \(\mathbb{W}\) such that \(\mathbb{S}_2=\mathbb{S}_1 \cup{\{\vec{\beta}_2\}}\)
        is independent. If we continue in this way, then (in not more than \(\dim{\mathbb{V}}\) steps) we reach a set 
        \[\mathbb{S}_m=\mathbb{S}_0 \cup{\{\vec{\beta}_1, \cdots, \vec{\beta}_m\}}\] 
    \end{proof}
    \setcounter{corollary}{0}
    \begin{corollary}
        If \(\mathbb{W}\) is a proper subspace of a finite-dimensional vector space 
        \(\mathbb{V}\), then \(\mathbb{W}\) is finite-dimensional and \(\dim{\mathbb{W}} < \dim{\mathbb{V}}\).
    \end{corollary}
    \begin{proof}
        We may suppose \(\mathbb{W}\) contains a vector \(\vec{\alpha} \neq 0\). By 
        Theorem 5 and its proof, there is a basis of \(\mathbb{W}\) containing \(\vec{\alpha}\)
        which contains no more than \(\dim{\mathbb{V}}\) elements. Hence \(\mathbb{W}\) is 
        finite-dimensional, and \(\dim{\mathbb{W}} \leq \dim{\mathbb{V}}\). Since \(\mathbb{W}\)
        is a proper subspace, there is a vector \(\vec{\beta}\) which is not in \(\mathbb{W}\).
        Adjoining \(\vec{\beta}\) to any basis in \(\mathbb{W}\), we obtain a linearly 
        independent subset of \(\mathbb{V}\). Thus \(\dim{\mathbb{W}} < \dim{\mathbb{V}}\).
    \end{proof}
    \begin{corollary}
        In a finite-dimensional vector space \(\mathbb{V}\) every non-empty linearly 
        independent set of vectors is part of a basis.
    \end{corollary}
    \begin{corollary}
        Let \(\mathbf{A}\) be an \(n \times n\) matrix over a field \(\mathbb{F}\), 
        and suppose the row vectors of \(\mathbf{A}\) form a linearly independent set 
        of vectors in \(\mathbb{F}^n\). Then \(\mathbf{A}\) is invertible. 
    \end{corollary}
    \begin{proof}
        Let \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\) be the row 
        vectors of \(\mathbf{A}\), and suppose \(\mathbb{W}\) is the subspace of 
        \(\mathbb{F}^n\) spanned by \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\). 
        Since \(\vec{\alpha}_1, \vec{\alpha}_2, \cdots, \vec{\alpha}_n\) are linearly 
        independent, the dimension of \(\mathbb{W}\) is \(n\). Corollary 1 shows that 
        \(\mathbb{W}=\mathbb{F}^n\). Hence there exist scalars \(B_{ij}\) in \(\mathbb{F}\) 
        such that 
        \begin{equation*}
            \vec{\epsilon} = \sum_{j=1}^{n} B_{ij}\vec{\alpha}_j,\ \ \ \ \ \ \ \ 1 \leq i \leq n
        \end{equation*}
        where \(\{\vec{\epsilon}_1, \vec{\epsilon}_2, \cdots, \vec{\epsilon}_n\}\) is the 
        standard basis of \(\mathbb{F}^n\). Thus for the matrix \(\mathbf{B}\) with the 
        entries \(B_{ij}\) we have \[\mathbf{B}\mathbf{A}=\mathbf{I}\]
    \end{proof}
    \begin{theorem}
        If \(\mathbb{W}_1\) and \(\mathbb{W}_2\) are finite-dimensional subspaces of 
        a vector space \(\mathbb{V}\), then \(\mathbb{W}_1+\mathbb{W}_2\) is finite-dimesional 
        and 
        \begin{equation*}
            \dim{\mathbb{W}_1} + \dim{\mathbb{W}_2} = \dim{(\mathbb{W}_1 \cap \mathbb{W}_2)} + \dim{(\mathbb{W}_1 + \mathbb{W}_2)}
        \end{equation*}
    \end{theorem}
    \begin{proof}
        By Theorem 5 and its corollaries, \(\mathbb{W}_1 \cap \mathbb{W}_2\) has a finite 
        basis \(\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k\}\) which is part of a basis 
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\beta}_1, \cdots, \vec{\beta}_m\} \ for \  \mathbb{W}_1\]
        and part of a basis 
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\gamma}_1, \cdots, \vec{\gamma}_n\} \ for \  \mathbb{W}_2\]
        The subspace \(\mathbb{W}_1 + \mathbb{W}_2\) is spanned by the vectors 
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\beta}_1, \cdots, \vec{\beta}_m, \ \vec{\gamma}_1, \cdots, \vec{\gamma}_n\}\]
        And these vectors form an independent set. For suppose
        \begin{equation*}
            \sum{x_i \vec{\alpha}_i} + \sum{y_j \vec{\beta}_j} + \sum{z_r \vec{\gamma}_r} = 0
        \end{equation*}
        Then 
        \begin{equation*}
            - \sum{z_r \vec{\gamma}_r} = \sum{x_i \vec{\alpha}_i} + \sum{y_j \vec{\beta}_j} 
        \end{equation*}
        which shows that \(\sum{z_r \vec{\gamma}_r}\) belongs to \(\mathbb{W}_1\). As \(\sum{z_r \vec{\gamma}_r}\)
        also belongs to \(\mathbb{W}_2\) it follows that 
        \[\sum{z_r \vec{\gamma}_r} = \sum{c_i \vec{\alpha}_i}\]
        for certain scalars \(c_1, \cdots, c_k\). Because the set 
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\gamma}_1, \cdots, \vec{\gamma}_n\}\]
        is independent, each of the scalars \(z_r=0\). Thus
        \[\sum{x_i \vec{\alpha}_i} + \sum{y_j \vec{\beta}_j}=0\]
        and since 
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\beta}_1, \cdots, \vec{\beta}_m\}\]
        is also an independent set, each \(x_i=0\) and each \(y_j=0\). Thus,
        \[\{\vec{\alpha}_1, \cdots, \vec{\alpha}_k, \ \vec{\beta}_1, \cdots, \vec{\beta}_m, \ \vec{\gamma}_1, \cdots, \vec{\gamma}_n\}\]
        is a basis for \(\mathbb{W}_1 + \mathbb{W}_2\). Finally 
        \begin{equation*}
            \begin{split}
                \dim{\mathbb{W}_1} + \dim{\mathbb{W}_2} &= (k+m)+(k+n) \\ 
                                                        &= k + (m + k + n) \\ 
                                                        &= \dim{(\mathbb{W}_1 \cap \mathbb{W}_2)} + \dim{(\mathbb{W}_1 + \mathbb{W}_2)}
            \end{split}
        \end{equation*}
    \end{proof}

\end{document}